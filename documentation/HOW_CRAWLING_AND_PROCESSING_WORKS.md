# ğŸ•·ï¸ How Web Crawling & Document Processing Works

> A beginner-friendly guide to the tools we use to build our knowledge base.

---

## ğŸ“– What Are These Tools?

To build HÃ¥faGPT's knowledge base (45,000+ chunks), we need to:

1. **Crawl websites** â†’ Get content from web pages
2. **Process PDFs** â†’ Extract text from PDF documents
3. **Chunk text** â†’ Split into digestible pieces for the AI

Here are the tools we use:

| Tool | Purpose | Why We Use It |
|------|---------|---------------|
| **Crawl4AI** | Web scraping | Async, JavaScript rendering, Markdown output |
| **Docling** | PDF processing | Better structure understanding than PyPDF2 |
| **PyPDF2** | PDF fallback | Simple, reliable, no dependencies |
| **ImprovedChunker** | Text splitting | Token-aware, respects document structure |

---

## ğŸ•·ï¸ Crawl4AI - Web Scraping

### What is Crawl4AI?

[Crawl4AI](https://github.com/unclecode/crawl4ai) is an async web crawler that:
- Renders JavaScript (important for modern websites)
- Converts HTML to clean Markdown
- Handles pagination and navigation
- Runs in headless browser (no visible window)

### Basic Usage

```python
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def crawl_page(url):
    # Configure the browser
    browser_config = BrowserConfig(
        headless=True,  # No visible browser window
    )
    
    # Configure the crawl
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,  # Always fetch fresh
    )
    
    # Run the crawler
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=url, config=run_config)
        
        if result.success:
            # result.markdown contains clean text!
            return result.markdown
        else:
            print(f"Failed: {result.error_message}")
            return None

# Run it
import asyncio
content = asyncio.run(crawl_page("https://example.com"))
print(content)
```

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CRAWL4AI FLOW                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     URL                    HEADLESS BROWSER              OUTPUT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ https://...  â”‚  â”€â”€â”€â”€â”€â”€â–¶  â”‚   Chromium       â”‚  â”€â”€â”€â–¶  â”‚   Markdown   â”‚
â”‚              â”‚           â”‚   (invisible)    â”‚        â”‚   Content    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚  1. Load page    â”‚
                           â”‚  2. Run JS       â”‚
                           â”‚  3. Wait for     â”‚
                           â”‚     content      â”‚
                           â”‚  4. Extract HTML â”‚
                           â”‚  5. Convert to   â”‚
                           â”‚     Markdown     â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Markdown?

Crawl4AI converts HTML to Markdown because:

```html
<!-- HTML (messy) -->
<h1>HÃ¥fa Adai</h1>
<p>This is a <strong>greeting</strong>.</p>
<a href="/about">Learn more</a>
```

```markdown
# HÃ¥fa Adai

This is a **greeting**.

[Learn more](/about)
```

âœ… Cleaner for AI to process
âœ… No HTML tags cluttering the text
âœ… Preserves structure (headings, lists, links)

### Site-Specific Crawlers

Different websites need different cleaning. We create custom crawlers:

```
crawlers/
â”œâ”€â”€ _template.py            # Starting point for new crawlers
â”œâ”€â”€ pacific_daily_news.py   # PDN bilingual columns
â”œâ”€â”€ iknm_kam_dictionary.py  # CNMI dictionary
â””â”€â”€ README.md               # How to create crawlers
```

**Example: PDN needs different cleaning than a dictionary site**

```python
# pacific_daily_news.py - Keep article content, remove ads
def clean_content(text):
    # Remove navigation
    text = re.sub(r'Skip to main content.*?\n', '', text)
    # Remove social buttons
    text = re.sub(r'Share on Facebook.*?\n', '', text)
    # Keep article body
    return text

# iknm_kam_dictionary.py - Keep dictionary entries
def clean_content(text):
    # Find where dictionary entries start
    if 'A - ' in text:
        text = text[text.index('A - '):]
    # Remove footer stats
    text = re.sub(r'Total number of entries:.*', '', text)
    return text
```

---

## ğŸ“„ Docling - PDF Processing

### What is Docling?

[Docling](https://github.com/DS4SD/docling) is IBM's document understanding library that:
- Extracts text with layout awareness
- Understands tables, headers, footers
- Preserves document structure
- Outputs clean Markdown

### Why Not Just PyPDF2?

| Feature | PyPDF2 | Docling |
|---------|--------|---------|
| Basic text | âœ… | âœ… |
| Tables | âŒ Garbled | âœ… Formatted |
| Multi-column | âŒ Mixed up | âœ… Correct order |
| Headers/Footers | âŒ Included | âœ… Can remove |
| Structure | âŒ Plain text | âœ… Markdown |

### Basic Usage

```python
from docling.document_converter import DocumentConverter

def process_pdf_with_docling(pdf_path):
    """Process PDF using Docling for better structure."""
    
    # Initialize converter
    converter = DocumentConverter()
    
    # Convert PDF to Docling document
    result = converter.convert(pdf_path)
    
    # Export as Markdown (preserves structure)
    markdown = result.document.export_to_markdown()
    
    return markdown

# Example
content = process_pdf_with_docling("chamorro_grammar.pdf")
print(content)
```

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DOCLING FLOW                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     PDF FILE              DOCLING PROCESSING           OUTPUT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“„ PDF      â”‚  â”€â”€â”€â”€â”€â”€â–¶ â”‚  1. PDF parsing  â”‚ â”€â”€â”€â–¶  â”‚   Markdown   â”‚
â”‚  (complex)   â”‚          â”‚  2. Layout detectâ”‚       â”‚   (clean)    â”‚
â”‚              â”‚          â”‚  3. Table extractâ”‚       â”‚              â”‚
â”‚  - Tables    â”‚          â”‚  4. Text order   â”‚       â”‚  # Heading   â”‚
â”‚  - Headers   â”‚          â”‚  5. Structure    â”‚       â”‚  | Table |   â”‚
â”‚  - Columns   â”‚          â”‚     recognition  â”‚       â”‚  Content...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fallback to PyPDF2

If Docling isn't available (it requires extra dependencies), we fall back:

```python
# src/utils/improved_chunker.py

class DoclingPDFProcessor:
    def process_pdf(self, pdf_path):
        try:
            # Try Docling first
            return self._process_with_docling(pdf_path)
        except:
            # Fallback to PyPDF2
            logger.warning("Docling failed, using PyPDF2")
            return self._process_with_pypdf(pdf_path)
    
    def _process_with_pypdf(self, pdf_path):
        """Simple PyPDF2 extraction."""
        from pypdf import PdfReader
        
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text
```

---

## âœ‚ï¸ ImprovedChunker - Text Splitting

### Why Chunk Text?

LLMs have context limits. We can't send 100 pages at once. Instead:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ORIGINAL DOCUMENT (10,000 tokens)                                     â”‚
â”‚                                                                        â”‚
â”‚  Chapter 1: Greetings                                                  â”‚
â”‚  HÃ¥fa Adai means hello...                                             â”‚
â”‚  Si Yu'os Ma'Ã¥se means thank you...                                   â”‚
â”‚                                                                        â”‚
â”‚  Chapter 2: Numbers                                                    â”‚
â”‚  Unu means one...                                                      â”‚
â”‚  ...                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHUNK 1        â”‚  â”‚  CHUNK 2        â”‚  â”‚  CHUNK 3        â”‚
â”‚  (~400 tokens)  â”‚  â”‚  (~400 tokens)  â”‚  â”‚  (~400 tokens)  â”‚
â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚
â”‚  Chapter 1:     â”‚  â”‚  Si Yu'os...    â”‚  â”‚  Chapter 2:     â”‚
â”‚  Greetings      â”‚  â”‚  (continues)    â”‚  â”‚  Numbers        â”‚
â”‚  HÃ¥fa Adai...   â”‚  â”‚                 â”‚  â”‚  Unu means...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Basic vs Improved Chunking

**Basic (character-based):**
```python
# Splits at exactly 1000 characters - might break mid-word!
chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]

# Problem: "HÃ¥fa A" | "dai means hello"  âŒ
```

**Improved (token-aware):**
```python
# Counts actual tokens, respects boundaries
chunker = ImprovedChunker(max_tokens=400)
chunks = chunker.chunk_text(text)

# Result: "HÃ¥fa Adai means hello..." | "Si Yu'os Ma'Ã¥se..."  âœ…
```

### How ImprovedChunker Works

```python
# src/utils/improved_chunker.py

class ImprovedChunker:
    def __init__(self, max_tokens=400, overlap_tokens=50):
        """
        max_tokens: Maximum tokens per chunk (400 is good for embeddings)
        overlap_tokens: Tokens to repeat between chunks (for context)
        """
        self.max_tokens = max_tokens
        self.overlap_tokens = overlap_tokens
        
        # Use actual tokenizer (not character estimation)
        self.tokenizer = AutoTokenizer.from_pretrained(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
    
    def count_tokens(self, text):
        """Count REAL tokens, not characters."""
        return len(self.tokenizer.encode(text))
    
    def chunk_text(self, text):
        """Split respecting semantic boundaries."""
        
        # 1. Split into paragraphs first
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = self.count_tokens(para)
            
            # If adding this paragraph exceeds limit, save current chunk
            if current_tokens + para_tokens > self.max_tokens:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_tokens = 0
            
            current_chunk.append(para)
            current_tokens += para_tokens
        
        # Don't forget the last chunk!
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
```

### Token Overlap

Why overlap? So chunks don't lose context at boundaries:

```
Without overlap:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "HÃ¥fa Adai is   â”‚  â”‚ "You can also   â”‚
â”‚ a greeting."    â”‚  â”‚ say HÃ¥fa..."    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†‘ Gap - context lost!

With 50-token overlap:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "HÃ¥fa Adai is   â”‚
â”‚ a greeting.     â”‚
â”‚ You can also"   â”‚  â† Overlaps with next chunk
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ "a greeting.    â”‚  â† Repeats ending
         â”‚ You can also    â”‚
         â”‚ say HÃ¥fa..."    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ How It All Fits Together

### Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KNOWLEDGE BASE BUILDING PIPELINE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 SOURCE                 EXTRACTION              CHUNKING            DATABASE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Websiteâ”‚ â”€â”€Crawl4AIâ”€â”€â”‚ Markdown â”‚â”€â”€Chunkerâ”€â”€â”‚ 400-tokenâ”‚â”€â”€Embedâ”€â”‚ PGVector â”‚
â”‚ (HTML) â”‚             â”‚ Text     â”‚           â”‚ Chunks   â”‚        â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚          â”‚
                                                                   â”‚  45,183  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  chunks  â”‚
â”‚  PDF   â”‚ â”€â”€Doclingâ”€â”€â”€â”‚ Markdown â”‚â”€â”€Chunkerâ”€â”€â”‚ 400-tokenâ”‚â”€â”€Embedâ”€â”‚          â”‚
â”‚        â”‚             â”‚ Text     â”‚           â”‚ Chunks   â”‚        â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code Example: Full Process

```python
# 1. Crawl a website
from crawl4ai import AsyncWebCrawler

async def crawl_site(url):
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url)
        return result.markdown

# 2. Process a PDF  
from src.utils.improved_chunker import DoclingPDFProcessor

def process_pdf(path):
    processor = DoclingPDFProcessor()
    return processor.process_pdf(path)

# 3. Chunk the content
from src.utils.improved_chunker import ImprovedChunker

def chunk_content(text):
    chunker = ImprovedChunker(max_tokens=400)
    return chunker.chunk_text(text)

# 4. Add to database
from src.rag.manage_rag_db import RAGDatabaseManager

def add_to_rag(chunks, source_name):
    manager = RAGDatabaseManager()
    manager.add_documents(chunks, metadata={"source": source_name})

# Complete pipeline
async def ingest_website(url, source_name):
    text = await crawl_site(url)           # Step 1: Crawl
    chunks = chunk_content(text)           # Step 2: Chunk
    add_to_rag(chunks, source_name)        # Step 3: Store
    print(f"Added {len(chunks)} chunks from {source_name}")
```

---

## ğŸ“ Key Files

| File | Purpose |
|------|---------|
| `crawlers/_template.py` | Template for new website crawlers |
| `crawlers/pacific_daily_news.py` | PDN-specific crawler |
| `crawlers/iknm_kam_dictionary.py` | CNMI dictionary crawler |
| `src/utils/improved_chunker.py` | Token-aware chunking + Docling |
| `src/rag/manage_rag_db.py` | Add/remove documents from RAG |
| `src/crawlers/crawl_website.py` | Generic website crawler |

---

## ğŸš€ Try It Yourself!

### 1. Crawl a Website

```bash
cd HafaGPT-API

# Test mode (preview only, doesn't add to database)
uv run python crawlers/pacific_daily_news.py --test "https://www.guampdn.com/..."

# Actually add to database
uv run python crawlers/pacific_daily_news.py "https://www.guampdn.com/..."
```

### 2. Process a PDF

```bash
cd HafaGPT-API
uv run python -c "
from src.utils.improved_chunker import DoclingPDFProcessor

processor = DoclingPDFProcessor()
content = processor.process_pdf('path/to/your.pdf')
print(content[:1000])  # First 1000 chars
"
```

### 3. Chunk Some Text

```bash
cd HafaGPT-API
uv run python -c "
from src.utils.improved_chunker import ImprovedChunker

chunker = ImprovedChunker(max_tokens=400)
text = '''
# Chapter 1
HÃ¥fa Adai is the Chamorro greeting...

# Chapter 2  
Numbers in Chamorro...
'''
chunks = chunker.chunk_text(text)
for i, chunk in enumerate(chunks):
    print(f'--- Chunk {i+1} ---')
    print(chunk[:200])
"
```

---

## ğŸ”§ Installation

These tools are already in our dependencies, but if you need them elsewhere:

```bash
# Crawl4AI (web scraping)
pip install crawl4ai

# Docling (PDF processing) - optional, heavy
pip install docling

# PyPDF2 (PDF fallback) - lightweight
pip install pypdf2

# Transformers (for tokenizer)
pip install transformers
```

âš ï¸ **Note:** Docling is memory-intensive (~500MB). We use it locally but not in production.

---

## ğŸ’¡ Tips

1. **Always test crawlers** with `--test` flag before adding to database
2. **Check chunk sizes** - 400 tokens is good for embedding models
3. **Clean aggressively** - navigation, ads, and junk hurt RAG quality
4. **Document sources** - update `SOURCES.md` when adding new content
5. **Use metadata** - track source, date, priority for each chunk

---

## ğŸ“š Learn More

- [Crawl4AI GitHub](https://github.com/unclecode/crawl4ai)
- [Docling GitHub](https://github.com/DS4SD/docling)
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [Token Counting Explained](https://platform.openai.com/tokenizer)

---

**Happy crawling!** ğŸ•·ï¸ğŸŒº

