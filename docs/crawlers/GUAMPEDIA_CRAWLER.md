# ğŸŒ Guampedia Crawler - Setup Guide

This guide shows you how to crawl Guampedia.com to enhance HÃ¥faGPT's knowledge of Chamorro culture, language, and history.

---

## ğŸ¯ What You'll Get

By crawling Guampedia, you'll add:

- âœ… **300-500 pages** of authoritative Chamorro cultural content
- âœ… **Folktales** - Legends of Sirena, Puntan & Fu'una, Gadao's Strength, etc.
- âœ… **Language resources** - Orthography, directional terminology, seafaring lexicon
- âœ… **History** - Ancient Chamorro life, Spanish era, WWII, modern Guam
- âœ… **Culture** - Traditional practices, nobenas, food, art, music
- âœ… **Biographies** - Important Chamorro figures and educators

---

## ğŸš€ Quick Start

### Option 1: Test Crawl First (Recommended)

Start with a small test to verify everything works:

```bash
cd HafaGPT-API
./crawl_guampedia_test.sh
```

This crawls **only the Chamorro Folktales section** (~10-20 pages, 2-5 minutes).

**Test the results:**
```bash
# Start the chatbot
uv run python chamorro-chatbot-3.0.py

# Ask questions like:
# - "Tell me about the legend of Sirena"
# - "What are some Chamorro folktales?"
# - "Tell me the story of Puntan and Fu'una"
```

If the results look good, proceed to the full crawl!

---

### Option 2: Full Site Crawl

Crawl the **entire Guampedia site**:

```bash
cd HafaGPT-API
./crawl_guampedia.sh
```

**Stats:**
- â±ï¸  **Time:** 2-4 hours
- ğŸ“„ **Pages:** 300-500
- ğŸ’° **Cost:** ~$2-5 (OpenAI embeddings)

**Important:**
- âœ… Make sure your local `.env` points to the **Neon production database**
- âœ… The script stays within `guampedia.com` domain only
- âœ… It automatically deduplicates (won't index the same page twice)
- âœ… You can safely stop/restart (it checks metadata before re-crawling)

---

## ğŸ› ï¸ Advanced Usage

### Manual Crawl with Custom Settings

```bash
# Crawl with custom depth and page limits
uv run python crawl_website.py https://www.guampedia.com/ \
  --max-depth 0 \
  --max-pages 500

# Crawl a specific section only
uv run python crawl_website.py https://www.guampedia.com/language/ \
  --max-depth 2
```

**Parameters:**
- `--max-depth 0` = Unlimited depth (crawl everything)
- `--max-depth 2` = Only go 2 links deep
- `--max-pages 500` = Stop after 500 pages
- `--same-domain-only` = Stay on guampedia.com (always enabled)

---

## ğŸ“Š Monitoring Progress

While crawling, you'll see:

```
[1] Crawling: https://www.guampedia.com/ (depth 0)
    âœ… Success (15234 chars)
    ğŸ“‹ Found 42 new links

[2] Crawling: https://www.guampedia.com/categories/ (depth 1)
    âœ… Success (8765 chars)
    ğŸ“‹ Found 38 new links

ğŸ“Š Progress: 10 pages crawled, 68 in queue
```

---

## ğŸ” Verify Indexed Content

Check what's in your database:

```bash
# List all indexed websites
uv run python manage_rag_db.py list

# Search for specific content
uv run python manage_rag_db.py search "Chamorro folktales"

# See total chunk count
uv run python manage_rag_db.py stats
```

---

## âš ï¸ Important Notes

### Before You Start:

1. **Point to Production DB:**
   ```bash
   # In HafaGPT-API/.env
   DATABASE_URL=postgresql://your-neon-url
   ```

2. **Check OpenAI API Key:**
   ```bash
   # Make sure this is set in .env
   OPENAI_API_KEY=sk-...
   ```

3. **Ensure Dependencies:**
   ```bash
   cd HafaGPT-API
   uv sync
   ```

### During Crawl:

- âœ… You can safely stop with `Ctrl+C` and resume later
- âœ… The crawler respects rate limits (0.5s delay between requests)
- âœ… Failed pages are logged but won't stop the crawl
- âœ… Progress updates every 10 pages

### After Crawl:

- âœ… Metadata is saved to `rag_metadata.json`
- âœ… Test your chatbot with Guampedia-specific questions
- âœ… Run again quarterly to pick up new Guampedia content

---

## ğŸ› Troubleshooting

**"No content found"**
- Check your internet connection
- Verify the URL is accessible in a browser
- Try the test crawl first

**"Failed to crawl"**
- Some pages might have JavaScript that blocks crawlers
- These are logged and skipped automatically

**"Database connection error"**
- Verify `DATABASE_URL` in `.env` is correct
- Check Neon database is accessible

**"OpenAI API error"**
- Verify `OPENAI_API_KEY` in `.env`
- Check your OpenAI account has credits

---

## ğŸ“š Next Steps

After crawling Guampedia:

1. **Test the chatbot** with cultural questions
2. **Add more sources** (other Chamorro websites, PDFs)
3. **Monitor user feedback** to find gaps in knowledge
4. **Re-crawl quarterly** to keep content fresh

---

**Hafa Adai!** ğŸŒº

