# HÃ¥faGPT API - Codebase Structure

This document outlines the clean, organized structure of the HÃ¥faGPT API codebase.

## ğŸ“ Directory Structure

```
HafaGPT-API/
â”œâ”€â”€ ğŸ“ api/                          # FastAPI application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                     # FastAPI app & routes
â”‚   â”œâ”€â”€ chatbot_service.py          # Chatbot logic & response generation
â”‚   â”œâ”€â”€ conversations.py            # Conversation CRUD operations
â”‚   â”œâ”€â”€ models.py                   # Pydantic models
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ“ src/                          # All Python source code
â”‚   â”œâ”€â”€ ğŸ“ crawlers/                # Web crawlers for data ingestion
â”‚   â”‚   â”œâ”€â”€ crawl_website.py       # Generic website crawler (Guampedia)
â”‚   â”‚   â””â”€â”€ crawl_lengguahita.py   # Lengguahi-ta specific crawler
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ importers/               # Data importers
â”‚   â”‚   â”œâ”€â”€ import_dictionary.py   # Import dictionary JSON files
â”‚   â”‚   â””â”€â”€ import_news_articles.py # Import news articles
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ rag/                     # RAG (Retrieval-Augmented Generation) system
â”‚   â”‚   â”œâ”€â”€ chamorro_rag.py        # RAG search & retrieval logic
â”‚   â”‚   â”œâ”€â”€ manage_rag_db.py       # RAG database management
â”‚   â”‚   â””â”€â”€ web_search_tool.py     # Web search integration
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ utils/                   # Utility scripts
â”‚       â”œâ”€â”€ inspect_rag_db.py      # Database inspection tool
â”‚       â”œâ”€â”€ improved_chunker.py    # Smart text chunking
â”‚       â”œâ”€â”€ sync_metadata.py       # Metadata synchronization
â”‚       â”œâ”€â”€ update_metadata_from_db.py # Update metadata from DB
â”‚       â””â”€â”€ find_max_id.py         # Find max IDs in DB
â”‚
â”œâ”€â”€ ğŸ“ scripts/                      # All shell scripts
â”‚   â”œâ”€â”€ ğŸ“ crawlers/                # Crawler wrapper scripts
â”‚   â”‚   â”œâ”€â”€ crawl_guampedia.sh     # Full Guampedia crawl
â”‚   â”‚   â”œâ”€â”€ crawl_guampedia_test.sh # Test Guampedia crawl
â”‚   â”‚   â”œâ”€â”€ crawl_guampedia_micro_test.sh # Micro test (5-10 pages)
â”‚   â”‚   â”œâ”€â”€ crawl_lengguahita.sh   # Full Lengguahi-ta crawl
â”‚   â”‚   â”œâ”€â”€ crawl_lengguahita_test.sh # Test Lengguahi-ta crawl
â”‚   â”‚   â”œâ”€â”€ monitor_guampedia_crawl.sh # Monitor crawl progress
â”‚   â”‚   â””â”€â”€ crawl_pdn_batch.sh     # Pacific Daily News batch crawl
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ importers/               # Importer wrapper scripts
â”‚   â”‚   â”œâ”€â”€ download_dictionaries.sh # Download dictionary files
â”‚   â”‚   â”œâ”€â”€ import_dictionaries.sh   # Import dictionaries
â”‚   â”‚   â”œâ”€â”€ download_news_articles.sh # Download news articles
â”‚   â”‚   â””â”€â”€ import_news_articles.sh  # Import news articles
â”‚   â”‚
â”‚   â”œâ”€â”€ inspect_db.sh               # Database inspection
â”‚   â”œâ”€â”€ dev-network.sh              # Start dev server on network
â”‚   â””â”€â”€ start.sh                    # Start production server
â”‚
â”œâ”€â”€ ğŸ“ docs/                         # All documentation
â”‚   â”œâ”€â”€ ğŸ“ setup/                   # Setup & configuration docs
â”‚   â”‚   â”œâ”€â”€ AUTHENTICATION_STATUS.md
â”‚   â”‚   â”œâ”€â”€ CLERK_IMPLEMENTATION_GUIDE.md
â”‚   â”‚   â”œâ”€â”€ EMBEDDINGS_GUIDE.md
â”‚   â”‚   â””â”€â”€ CONVERSATION_ANALYTICS.md
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ crawlers/                # Crawler documentation
â”‚   â”‚   â”œâ”€â”€ GUAMPEDIA_CRAWLER.md
â”‚   â”‚   â”œâ”€â”€ GUAMPEDIA_SETUP_COMPLETE.md
â”‚   â”‚   â”œâ”€â”€ GUAMPEDIA_CRAWL_STATUS.md
â”‚   â”‚   â”œâ”€â”€ LENGGUAHITA_CRAWLER.md
â”‚   â”‚   â”œâ”€â”€ LENGGUAHITA_SETUP_COMPLETE.md
â”‚   â”‚   â””â”€â”€ CRAWLER_FIX_BATCH_WRITES.md
â”‚   â”‚
â”‚   â”œâ”€â”€ CODEBASE_STRUCTURE.md       # This file
â”‚   â”œâ”€â”€ DATA_IMPORT_MASTER_PLAN.md
â”‚   â”œâ”€â”€ RAG_PRIORITY_SYSTEM.md
â”‚   â””â”€â”€ PRIORITY_AND_TRACKING_EXPLAINED.md
â”‚
â”œâ”€â”€ ğŸ“ logs/                         # Log files
â”‚   â”œâ”€â”€ guampedia_crawl.log
â”‚   â”œâ”€â”€ chamoru_crawl.log
â”‚   â”œâ”€â”€ chamoru_extended_crawl.log
â”‚   â””â”€â”€ conversation_logs.jsonl
â”‚
â”œâ”€â”€ ğŸ“ data/                         # Data files
â”‚   â””â”€â”€ pdn_urls.txt
â”‚
â”œâ”€â”€ ğŸ“ tests/                        # Test files
â”‚   â”œâ”€â”€ test_system.py
â”‚   â””â”€â”€ chamorro-chatbot-3.0.py (legacy)
â”‚
â”œâ”€â”€ ğŸ“ alembic/                      # Database migrations
â”‚   â””â”€â”€ versions/
â”‚
â”œâ”€â”€ ğŸ“ backups/                      # Database backups
â”‚   â””â”€â”€ chamorro_rag_backup.sql
â”‚
â”œâ”€â”€ ğŸ“ knowledge_base/               # Source materials
â”‚   â”œâ”€â”€ chamorro_abbreviations.md
â”‚   â””â”€â”€ pdfs/
â”‚
â”œâ”€â”€ ğŸ“ archive/                      # Old code & experiments
â”‚   â”œâ”€â”€ ai-frontend/
â”‚   â”œâ”€â”€ crawl-scripts/
â”‚   â”œâ”€â”€ experimental-code/
â”‚   â”œâ”€â”€ learning-examples/
â”‚   â””â”€â”€ old-chatbot-versions/
â”‚
â”œâ”€â”€ alembic.ini                      # Alembic configuration
â”œâ”€â”€ pyproject.toml                   # Project dependencies (uv)
â”œâ”€â”€ requirements.txt                 # Production dependencies (pip)
â”œâ”€â”€ render.yaml                      # Render deployment config
â””â”€â”€ README.md                        # Main README

```

## ğŸ¯ Key Design Principles

### 1. **Clear Separation of Concerns**
- `api/` â†’ Web API layer
- `src/` â†’ Business logic
- `scripts/` â†’ Automation & utilities
- `docs/` â†’ Documentation

### 2. **Logical Grouping**
- Crawlers together
- Importers together
- RAG system together
- Utilities together

### 3. **Easy Navigation**
- Files are named descriptively
- Directories are organized by function
- Documentation is grouped by topic

### 4. **Import Paths**
All imports use absolute paths from the project root:

```python
# âœ… Correct
from src.rag.manage_rag_db import RAGDatabaseManager
from src.utils.improved_chunker import create_improved_chunker

# âŒ Old (no longer used)
from manage_rag_db import RAGDatabaseManager
from improved_chunker import create_improved_chunker
```

### 5. **Script Execution**
All scripts automatically change to the project root:

```bash
# Inside any script in scripts/ subdirectories:
cd "$(dirname "$0")/../.." || exit 1
uv run python src/crawlers/crawl_website.py https://example.com
```

## ğŸ“ Running Scripts

### Crawlers
```bash
# From project root
cd HafaGPT-API

# Run crawlers
./scripts/crawlers/crawl_guampedia_micro_test.sh
./scripts/crawlers/crawl_lengguahita_test.sh

# Inspect database
./scripts/inspect_db.sh
```

### Development
```bash
# Start API server on network
./scripts/dev-network.sh

# Run tests
pytest tests/
```

## ğŸ”„ Migration Notes

This structure was implemented on **2025-11-16** to improve code organization and maintainability.

### Changes Made:
1. âœ… Moved all Python source to `src/` with logical subdirectories
2. âœ… Moved all shell scripts to `scripts/` with logical subdirectories
3. âœ… Moved all documentation to `docs/` with logical subdirectories
4. âœ… Moved all logs to `logs/`
5. âœ… Moved all data files to `data/`
6. âœ… Updated all imports to use absolute paths (`src.rag.*`)
7. âœ… Updated all scripts to use correct paths and change to project root
8. âœ… Tested API, crawlers, and utilities - all working!

### No Breaking Changes:
- API still runs on port 8000
- Database connections unchanged
- Environment variables unchanged
- All functionality preserved

## ğŸš€ Next Steps

The codebase is now ready for:
- Adding new crawlers (add to `src/crawlers/`)
- Adding new importers (add to `src/importers/`)
- Adding new RAG features (add to `src/rag/`)
- Adding new utilities (add to `src/utils/`)
- Writing unit tests (add to `tests/`)

---

**Last Updated:** 2025-11-16  
**Status:** âœ… Complete & Tested
